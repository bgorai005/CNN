{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torchvision import models\nimport torch.nn as nn\n\ndef pretrain_model(freeze_percent=0.0, freeze_all_except_last_layer=False, num_classes=10):\n    \"\"\"\n    Loads pretrained ResNet50, applies fine-tuning strategy, and replaces the final layer.\n\n    Parameters:\n    - freeze_percent (float): Fraction of layers to freeze (0.0 to 1.0).\n    - freeze_all_except_last_layer (bool): If True, freezes all layers except the last fully connected layer.\n    - num_classes (int): Number of output classes for classification.\n\n    Returns:\n    - model (nn.Module): Modified ResNet50 model.\n    \"\"\"\n    model = models.resnet50(pretrained=True)\n\n    if freeze_all_except_last_layer:\n        for param in model.parameters():\n            param.requires_grad = False\n        for param in model.fc.parameters():\n            param.requires_grad = True\n    elif freeze_percent > 0.0:\n        all_params = list(model.parameters())\n        freeze_until = int(len(all_params) * freeze_percent)\n        for i, param in enumerate(all_params):\n            param.requires_grad = i >= freeze_until\n\n    # Replace final classification layer\n    in_features = model.fc.in_features\n    model.fc = nn.Linear(in_features, num_classes)\n\n    return model\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:44:15.680180Z","iopub.execute_input":"2025-04-17T12:44:15.680427Z","iopub.status.idle":"2025-04-17T12:44:22.396750Z","shell.execute_reply.started":"2025-04-17T12:44:15.680404Z","shell.execute_reply":"2025-04-17T12:44:22.396179Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import ImageFolder\ndef data_load(data_dir, batch_size=64, val_split=0.2, data_augmentation=True):\n    train_transforms = transforms.Compose([\n        transforms.RandomResizedCrop(256),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n        transforms.RandomRotation(20),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ]) if data_augmentation else transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ])\n\n    val_transforms = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5]*3, [0.5]*3)\n    ])\n\n    dataset = datasets.ImageFolder(root=data_dir, transform=train_transforms)\n    val_size = int(val_split * len(dataset))\n    train_size = len(dataset) - val_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n    val_dataset.dataset.transform = val_transforms\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_loader, val_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:44:22.398004Z","iopub.execute_input":"2025-04-17T12:44:22.398321Z","iopub.status.idle":"2025-04-17T12:44:22.404887Z","shell.execute_reply.started":"2025-04-17T12:44:22.398303Z","shell.execute_reply":"2025-04-17T12:44:22.404053Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def test_data_load(test_data_dir, data_augmentation='No'):\n    # Transforms for resizing, normalization, etc.\n    resize = transforms.Resize((256, 256))\n    convert_to_tensor = transforms.ToTensor()\n    normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    resize_crop = transforms.RandomResizedCrop(256)\n    h_flip = transforms.RandomHorizontalFlip()\n    color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n    rand_rotation = transforms.RandomRotation(20)\n\n    if data_augmentation == 'Yes':\n        transform_img = transforms.Compose([\n            resize_crop,\n            h_flip,\n            color_jitter,\n            rand_rotation,\n            convert_to_tensor,\n            normalize\n        ])\n    else:\n        transform_img = transforms.Compose([\n            resize,\n            convert_to_tensor,\n            normalize\n        ])\n\n    # Load dataset\n    test_data = ImageFolder(root=test_data_dir, transform=transform_img)\n    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n\n    return test_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:44:34.821034Z","iopub.execute_input":"2025-04-17T12:44:34.821690Z","iopub.status.idle":"2025-04-17T12:44:34.826762Z","shell.execute_reply.started":"2025-04-17T12:44:34.821666Z","shell.execute_reply":"2025-04-17T12:44:34.826119Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n# Define the device globally\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef model_train_val(model, train_loader, val_loader, epochs=5, learning_rate=1e-3):\n    \"\"\"\n    Trains and validates the CNN model.\n    \"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n\n        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n        for inputs, labels in loop:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            loop.set_postfix(loss=loss.item(), acc=100. * correct / total)\n\n        train_loss /= total\n        train_acc = 100. * correct / total\n\n        # Validation Phase\n        model.eval()\n        val_loss = 0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n        val_loss /= total\n        val_acc = 100. * correct / total\n\n        print(f\"\\nEpoch {epochs}/{epochs} => \"\n              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n\n        \n        wandb.log({\n            'epoch+1': epochs + 1,\n            'Train Loss': train_loss,\n            'Train Accuracy': train_acc,\n            'Validation Loss': val_loss,\n            'Validation Accuracy': val_acc\n        })\n\n    print(\"Training complete!\")\n    return model  \n\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:44:39.211281Z","iopub.execute_input":"2025-04-17T12:44:39.211546Z","iopub.status.idle":"2025-04-17T12:44:39.289639Z","shell.execute_reply.started":"2025-04-17T12:44:39.211526Z","shell.execute_reply":"2025-04-17T12:44:39.288807Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import wandb\nwandb.login(key='594642013968a68e466138e783dcece6765c43b9')\n# Initialize your W&B run\nwandb.init(\n    project=\"DL_Assignment_2\",         # Change this to your actual project name\n    name=\"pre_trainedmodel_rs_net\",  # Optional: name of this run\n    \n)\nmodel=pretrain_model(freeze_percent=0.8, freeze_all_except_last_layer=False, num_classes=10)\nmodel=model.to(device)\ntrain_dir = '/kaggle/input/inaturalist/inaturalist_12K/train'\ntrain_loader,val_loader = data_load(train_dir, data_augmentation=True)\ntrained_model =  model_train_val(model, train_loader, val_loader, epochs=5)\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:44:44.234684Z","iopub.execute_input":"2025-04-17T12:44:44.235379Z","iopub.status.idle":"2025-04-17T12:51:14.534412Z","shell.execute_reply.started":"2025-04-17T12:44:44.235353Z","shell.execute_reply":"2025-04-17T12:51:14.533702Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbgorai005\u001b[0m (\u001b[33mbgorai005-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250417_124452-t8umlqn8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bgorai005-iit-madras/Dl_assignment_2/runs/t8umlqn8' target=\"_blank\">pre_trainedmodel_rs_net</a></strong> to <a href='https://wandb.ai/bgorai005-iit-madras/Dl_assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bgorai005-iit-madras/Dl_assignment_2' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/Dl_assignment_2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bgorai005-iit-madras/Dl_assignment_2/runs/t8umlqn8' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/Dl_assignment_2/runs/t8umlqn8</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 165MB/s] \nEpoch 1/5 [Train]: 100%|██████████| 125/125 [01:17<00:00,  1.62it/s, acc=65.4, loss=1.11] \nEpoch 1/5 [Val]: 100%|██████████| 32/32 [00:19<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/5 => Train Loss: 1.0353, Train Acc: 65.40%, Val Loss: 0.9014, Val Acc: 71.59%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5 [Train]: 100%|██████████| 125/125 [00:52<00:00,  2.38it/s, acc=79.7, loss=0.722]\nEpoch 2/5 [Val]: 100%|██████████| 32/32 [00:12<00:00,  2.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/5 => Train Loss: 0.6168, Train Acc: 79.71%, Val Loss: 0.9647, Val Acc: 70.09%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5 [Train]: 100%|██████████| 125/125 [00:53<00:00,  2.33it/s, acc=85, loss=0.498]  \nEpoch 3/5 [Val]: 100%|██████████| 32/32 [00:12<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/5 => Train Loss: 0.4635, Train Acc: 85.01%, Val Loss: 0.9399, Val Acc: 72.34%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5 [Train]: 100%|██████████| 125/125 [00:52<00:00,  2.39it/s, acc=91.4, loss=0.238] \nEpoch 4/5 [Val]: 100%|██████████| 32/32 [00:12<00:00,  2.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/5 => Train Loss: 0.2557, Train Acc: 91.41%, Val Loss: 1.1880, Val Acc: 70.84%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5 [Train]: 100%|██████████| 125/125 [00:52<00:00,  2.37it/s, acc=94.1, loss=0.309] \nEpoch 5/5 [Val]: 100%|██████████| 32/32 [00:13<00:00,  2.45it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/5 => Train Loss: 0.1791, Train Acc: 94.14%, Val Loss: 1.1267, Val Acc: 72.09%\nTraining complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁▄▆▇█</td></tr><tr><td>Train Loss</td><td>█▅▃▂▁</td></tr><tr><td>Validation Accuracy</td><td>▆▁█▃▇</td></tr><tr><td>Validation Loss</td><td>▁▃▂█▇</td></tr><tr><td>epoch+1</td><td>▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>94.1375</td></tr><tr><td>Train Loss</td><td>0.17913</td></tr><tr><td>Validation Accuracy</td><td>72.08604</td></tr><tr><td>Validation Loss</td><td>1.12672</td></tr><tr><td>epoch+1</td><td>6</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">pre_trainedmodel_rs_net</strong> at: <a href='https://wandb.ai/bgorai005-iit-madras/Dl_assignment_2/runs/t8umlqn8' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/Dl_assignment_2/runs/t8umlqn8</a><br> View project at: <a href='https://wandb.ai/bgorai005-iit-madras/Dl_assignment_2' target=\"_blank\">https://wandb.ai/bgorai005-iit-madras/Dl_assignment_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250417_124452-t8umlqn8/logs</code>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n# Define the device globally\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef evaluate_on_test(model, test_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    criterion = nn.CrossEntropyLoss()\n    test_loss = 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            test_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    test_loss /= total\n    test_acc = 100. * correct / total\n    print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n    return test_loss, test_acc\n\ntest_dir = '/kaggle/input/inaturalist/inaturalist_12K/val'\ntest_loader = test_data_load(test_dir, data_augmentation='No')\n# Call the test function\nevaluate_on_test(trained_model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T12:54:29.950385Z","iopub.execute_input":"2025-04-17T12:54:29.951112Z","iopub.status.idle":"2025-04-17T12:55:08.855090Z","shell.execute_reply.started":"2025-04-17T12:54:29.951082Z","shell.execute_reply":"2025-04-17T12:55:08.854489Z"}},"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 63/63 [00:38<00:00,  1.66it/s]","output_type":"stream"},{"name":"stdout","text":"\nTest Loss: 0.9777, Test Accuracy: 74.75%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(0.9776781388521194, 74.75)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}